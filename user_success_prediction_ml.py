import pandas as pd
import numpy as np

# Load the data
df = pd.read_csv("zervedataset1.csv")
print("Dataset loaded successfully")
print(f"Shape: {df.shape}")

# Check available columns
print("\nAvailable columns:")
print(df.columns.tolist())

# Parse timestamp
df['timestamp'] = pd.to_datetime(df['timestamp'], format='ISO8601')

# Identify the correct user identifier column
# Looking at the columns, we have: person_id, distinct_id, prop_$user_id, prop_userId, prop_user_id
# Use person_id as the main user identifier
user_id_col = 'person_id'

# Check if there's an event_type column, otherwise use 'event'
event_col = 'event' if 'event' in df.columns else 'event_type'

print(f"\nUsing '{user_id_col}' as user identifier")
print(f"Using '{event_col}' as event type")

# Sort by user and timestamp
df = df.sort_values([user_id_col, 'timestamp'])

# Create user-level features
user_features = df.groupby(user_id_col).agg(
    total_events=(event_col, 'count'),
    active_days=('timestamp', lambda x: x.dt.date.nunique()),
    first_activity=('timestamp', 'min'),
    last_activity=('timestamp', 'max')
).reset_index()

user_features['lifetime_days'] = (
    user_features['last_activity'] - user_features['first_activity']
).dt.days + 1

# Calculate days from start for each user
df['days_from_start'] = df.groupby(user_id_col)['timestamp'] \
    .transform(lambda x: (x - x.min()).dt.days)

# Extract early engagement features (first 7 days)
early_df = df[df['days_from_start'] <= 7]

early_features = early_df.groupby(user_id_col).agg(
    events_first_7_days=(event_col, 'count'),
    features_used_first_7_days=(event_col, 'nunique')
).reset_index()

# Calculate workflow entropy
from scipy.stats import entropy

def workflow_entropy(actions):
    probs = actions.value_counts(normalize=True)
    return entropy(probs)

workflow_features = df.groupby(user_id_col).agg(
    workflow_entropy=(event_col, workflow_entropy),
    unique_actions=(event_col, 'nunique')
).reset_index()

# Merge all features
features = user_features \
    .merge(early_features, on=user_id_col, how='left') \
    .merge(workflow_features, on=user_id_col, how='left')

features.fillna(0, inplace=True)

# Define success criteria
median_events = features['total_events'].median()
features['successful_user'] = (
    (features['active_days'] >= 20) &
    (features['total_events'] >= median_events)
).astype(int)

print(f"\nTotal users: {len(features)}")
print(f"Successful users: {features['successful_user'].sum()} ({features['successful_user'].mean()*100:.1f}%)")

# Train ML model
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report

X = features.drop([user_id_col, 'successful_user', 'first_activity', 'last_activity'], axis=1)
y = features['successful_user']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

model = RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1)
model.fit(X_train, y_train)

print("\n" + "="*50)
print("MODEL CLASSIFICATION REPORT")
print("="*50)
print(classification_report(y_test, model.predict(X_test)))

# Feature importance
importances = pd.Series(
    model.feature_importances_, index=X.columns
).sort_values(ascending=False)

print("\n" + "="*50)
print("TOP 10 FEATURE IMPORTANCES")
print("="*50)
for feat, imp in importances.head(10).items():
    print(f"{feat:35s}: {imp:.4f}")

prediction_model = model
import pandas as pd
import numpy as np

# Merge test data with features to access segmentation characteristics
test_indices = X_test.index
segment_data = features.loc[test_indices].copy()

# Add predictions to segment data
segment_data['y_true'] = y_test.values
segment_data['y_pred'] = prediction_model.predict(X_test)

# Create user segments with simpler percentile-based approach
# 1. Activity Level: Low, Medium, High based on total_events
activity_terciles = segment_data['total_events'].quantile([0.33, 0.67])
segment_data['activity_level'] = 'Medium'
segment_data.loc[segment_data['total_events'] <= activity_terciles[0.33], 'activity_level'] = 'Low'
segment_data.loc[segment_data['total_events'] > activity_terciles[0.67], 'activity_level'] = 'High'

# 2. Lifetime: Short, Medium, Long based on lifetime_days
lifetime_terciles = segment_data['lifetime_days'].quantile([0.33, 0.67])
segment_data['lifetime_segment'] = 'Medium'
segment_data.loc[segment_data['lifetime_days'] <= lifetime_terciles[0.33], 'lifetime_segment'] = 'Short'
segment_data.loc[segment_data['lifetime_days'] > lifetime_terciles[0.67], 'lifetime_segment'] = 'Long'

# 3. Engagement Pattern: based on workflow_entropy
entropy_terciles = segment_data['workflow_entropy'].quantile([0.33, 0.67])
segment_data['engagement_pattern'] = 'Balanced'
segment_data.loc[segment_data['workflow_entropy'] <= entropy_terciles[0.33], 'engagement_pattern'] = 'Focused'
segment_data.loc[segment_data['workflow_entropy'] > entropy_terciles[0.67], 'engagement_pattern'] = 'Exploratory'

# 4. Early Adopter: binary split on events_first_7_days
early_median = segment_data['events_first_7_days'].median()
segment_data['early_adopter'] = 'Low Early'
segment_data.loc[segment_data['events_first_7_days'] > early_median, 'early_adopter'] = 'High Early'

print("USER SEGMENTATION COMPLETE")
print("="*60)
print("\nSegment Definitions:")
print(f"\n1. Activity Level (based on total_events):")
print(f"   - Low: <={activity_terciles[0.33]:.1f} events")
print(f"   - Medium: {activity_terciles[0.33]:.1f} - {activity_terciles[0.67]:.1f} events")
print(f"   - High: >{activity_terciles[0.67]:.1f} events")

print(f"\n2. Lifetime (based on lifetime_days):")
print(f"   - Short: <={lifetime_terciles[0.33]:.0f} days")
print(f"   - Medium: {lifetime_terciles[0.33]:.0f} - {lifetime_terciles[0.67]:.0f} days")
print(f"   - Long: >{lifetime_terciles[0.67]:.0f} days")

print(f"\n3. Engagement Pattern (based on workflow_entropy):")
print(f"   - Focused: <={entropy_terciles[0.33]:.2f} (repetitive patterns)")
print(f"   - Balanced: {entropy_terciles[0.33]:.2f} - {entropy_terciles[0.67]:.2f}")
print(f"   - Exploratory: >{entropy_terciles[0.67]:.2f} (diverse exploration)")

print(f"\n4. Early Adopter (based on events_first_7_days):")
print(f"   - Low Early: <={early_median:.0f} events in first 7 days")
print(f"   - High Early: >{early_median:.0f} events in first 7 days")

print("\n" + "="*60)
print("SEGMENT DISTRIBUTION IN TEST SET")
print("="*60)
print(f"\nActivity Level:\n{segment_data['activity_level'].value_counts()}")
print(f"\nLifetime Segment:\n{segment_data['lifetime_segment'].value_counts()}")
print(f"\nEngagement Pattern:\n{segment_data['engagement_pattern'].value_counts()}")
print(f"\nEarly Adopter:\n{segment_data['early_adopter'].value_counts()}")

segmentation_df = segment_data
import pandas as pd
import numpy as np
from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score

# Calculate metrics for each segment
def calculate_segment_metrics(data, segment_col):
    """Calculate accuracy metrics for each segment"""
    results = []
    
    for segment in data[segment_col].unique():
        segment_mask = data[segment_col] == segment
        segment_subset = data[segment_mask]
        
        n_samples = len(segment_subset)
        y_true_seg = segment_subset['y_true']
        y_pred_seg = segment_subset['y_pred']
        
        # Calculate metrics
        acc = accuracy_score(y_true_seg, y_pred_seg)
        
        # Handle cases where only one class is present
        if len(y_true_seg.unique()) > 1 and len(y_pred_seg.unique()) > 1:
            precision = precision_score(y_true_seg, y_pred_seg, zero_division=0)
            recall = recall_score(y_true_seg, y_pred_seg, zero_division=0)
            f1 = f1_score(y_true_seg, y_pred_seg, zero_division=0)
        else:
            precision = recall = f1 = acc  # For single-class cases
        
        # Confusion matrix
        cm = confusion_matrix(y_true_seg, y_pred_seg)
        tn = cm[0, 0] if cm.shape[0] > 0 and cm.shape[1] > 0 else 0
        fp = cm[0, 1] if cm.shape[0] > 0 and cm.shape[1] > 1 else 0
        fn = cm[1, 0] if cm.shape[0] > 1 and cm.shape[1] > 0 else 0
        tp = cm[1, 1] if cm.shape[0] > 1 and cm.shape[1] > 1 else 0
        
        results.append({
            'segment': segment,
            'n_samples': n_samples,
            'accuracy': acc,
            'precision': precision,
            'recall': recall,
            'f1_score': f1,
            'true_negative': tn,
            'false_positive': fp,
            'false_negative': fn,
            'true_positive': tp
        })
    
    return pd.DataFrame(results).sort_values('segment')

# Calculate metrics for all segmentation dimensions
activity_metrics = calculate_segment_metrics(segmentation_df, 'activity_level')
activity_metrics['dimension'] = 'Activity Level'

lifetime_metrics = calculate_segment_metrics(segmentation_df, 'lifetime_segment')
lifetime_metrics['dimension'] = 'Lifetime'

engagement_metrics = calculate_segment_metrics(segmentation_df, 'engagement_pattern')
engagement_metrics['dimension'] = 'Engagement Pattern'

early_metrics = calculate_segment_metrics(segmentation_df, 'early_adopter')
early_metrics['dimension'] = 'Early Adopter'

# Combine all metrics
all_segment_metrics = pd.concat([
    activity_metrics, 
    lifetime_metrics, 
    engagement_metrics, 
    early_metrics
], ignore_index=True)

# Display results
print("="*80)
print("MODEL PERFORMANCE BY USER SEGMENT")
print("="*80)

for dim in ['Activity Level', 'Lifetime', 'Engagement Pattern', 'Early Adopter']:
    dim_data = all_segment_metrics[all_segment_metrics['dimension'] == dim]
    print(f"\n{dim.upper()}")
    print("-"*80)
    
    for _, row in dim_data.iterrows():
        print(f"\n{row['segment']}:")
        print(f"  Sample Size: {row['n_samples']}")
        print(f"  Accuracy: {row['accuracy']:.3f}")
        print(f"  Precision: {row['precision']:.3f} | Recall: {row['recall']:.3f} | F1: {row['f1_score']:.3f}")
        print(f"  Confusion Matrix: TN={row['true_negative']}, FP={row['false_positive']}, FN={row['false_negative']}, TP={row['true_positive']}")

print("\n" + "="*80)
print("KEY INSIGHTS")
print("="*80)

# Find segments with best/worst performance
best_accuracy = all_segment_metrics.loc[all_segment_metrics['accuracy'].idxmax()]
worst_accuracy = all_segment_metrics.loc[all_segment_metrics['accuracy'].idxmin()]

print(f"\nBest Performance: {best_accuracy['segment']} ({best_accuracy['dimension']})")
print(f"  Accuracy: {best_accuracy['accuracy']:.3f} | Sample Size: {best_accuracy['n_samples']}")

print(f"\nWorst Performance: {worst_accuracy['segment']} ({worst_accuracy['dimension']})")
print(f"  Accuracy: {worst_accuracy['accuracy']:.3f} | Sample Size: {worst_accuracy['n_samples']}")

# Calculate variance in accuracy across segments
for dim in ['Activity Level', 'Lifetime', 'Engagement Pattern', 'Early Adopter']:
    dim_data = all_segment_metrics[all_segment_metrics['dimension'] == dim]
    acc_variance = dim_data['accuracy'].std()
    print(f"\n{dim} - Accuracy Std Dev: {acc_variance:.3f}")

segment_performance_metrics = all_segment_metrics
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
import numpy as np

# Zerve design system colors
bg_color = '#1D1D20'
text_color = '#fbfbff'
grid_color = '#3a3a3f'
colors = ['#A1C9F4', '#FFB482', '#8DE5A1', '#FF9F9B', '#D0BBFF']

# Create comprehensive visualization of segment performance
fig = plt.figure(figsize=(16, 10))
fig.patch.set_facecolor(bg_color)

# 1. Accuracy by Segment (Grouped Bar Chart)
ax1 = plt.subplot(2, 2, 1)
ax1.set_facecolor(bg_color)

dimensions = ['Activity Level', 'Lifetime', 'Engagement Pattern', 'Early Adopter']
for idx, dim in enumerate(dimensions):
    dim_data = segment_performance_metrics[segment_performance_metrics['dimension'] == dim]
    segments = dim_data['segment'].values
    accuracies = dim_data['accuracy'].values
    
    x_pos = np.arange(len(segments)) + idx * 0.2
    ax1.bar(x_pos, accuracies, width=0.18, label=dim, color=colors[idx % len(colors)], alpha=0.9)

ax1.set_ylabel('Accuracy', fontsize=11, color=text_color, fontweight='bold')
ax1.set_title('Model Accuracy by Segment', fontsize=13, color=text_color, fontweight='bold', pad=15)
ax1.set_ylim([0.99, 1.001])
ax1.tick_params(axis='x', colors=text_color, labelsize=8, rotation=0)
ax1.tick_params(axis='y', colors=text_color, labelsize=9)
ax1.spines['bottom'].set_color(text_color)
ax1.spines['left'].set_color(text_color)
ax1.spines['top'].set_visible(False)
ax1.spines['right'].set_visible(False)
ax1.legend(loc='lower right', fontsize=8, facecolor=bg_color, edgecolor=text_color, labelcolor=text_color)
ax1.set_xticks([])

# 2. Sample Size Distribution
ax2 = plt.subplot(2, 2, 2)
ax2.set_facecolor(bg_color)

sample_sizes = segment_performance_metrics.groupby('dimension')['n_samples'].sum()
wedges, texts, autotexts = ax2.pie(
    sample_sizes.values,
    labels=sample_sizes.index,
    autopct='%1.0f%%',
    colors=colors[:len(sample_sizes)],
    startangle=90,
    textprops={'color': text_color, 'fontsize': 10}
)

for autotext in autotexts:
    autotext.set_color(bg_color)
    autotext.set_fontweight('bold')
    autotext.set_fontsize(9)

ax2.set_title('Sample Distribution by Segment Type', fontsize=13, color=text_color, fontweight='bold', pad=15)

# 3. Precision vs Recall by Segment
ax3 = plt.subplot(2, 2, 3)
ax3.set_facecolor(bg_color)

for idx, dim in enumerate(dimensions):
    dim_data = segment_performance_metrics[segment_performance_metrics['dimension'] == dim]
    ax3.scatter(
        dim_data['recall'], 
        dim_data['precision'],
        s=dim_data['n_samples'] * 0.5,
        alpha=0.7,
        color=colors[idx % len(colors)],
        label=dim,
        edgecolors=text_color,
        linewidths=0.5
    )

ax3.set_xlabel('Recall', fontsize=11, color=text_color, fontweight='bold')
ax3.set_ylabel('Precision', fontsize=11, color=text_color, fontweight='bold')
ax3.set_title('Precision vs Recall by Segment', fontsize=13, color=text_color, fontweight='bold', pad=15)
ax3.tick_params(axis='both', colors=text_color, labelsize=9)
ax3.spines['bottom'].set_color(text_color)
ax3.spines['left'].set_color(text_color)
ax3.spines['top'].set_visible(False)
ax3.spines['right'].set_visible(False)
ax3.legend(loc='lower left', fontsize=8, facecolor=bg_color, edgecolor=text_color, labelcolor=text_color)
ax3.set_xlim([0.7, 1.05])
ax3.set_ylim([0.95, 1.05])

# 4. Confusion Matrix Summary
ax4 = plt.subplot(2, 2, 4)
ax4.set_facecolor(bg_color)

segments_list = []
tp_list = []
tn_list = []
fp_list = []
fn_list = []

for idx, row in segment_performance_metrics.iterrows():
    segments_list.append(f"{row['segment'][:4]}")
    tp_list.append(row['true_positive'])
    tn_list.append(row['true_negative'])
    fp_list.append(row['false_positive'])
    fn_list.append(row['false_negative'])

x_pos = np.arange(len(segments_list))
width = 0.6

ax4.bar(x_pos, tn_list, width, label='True Negative', color=colors[2], alpha=0.9)
ax4.bar(x_pos, tp_list, width, bottom=tn_list, label='True Positive', color=colors[0], alpha=0.9)
ax4.bar(x_pos, fp_list, width, bottom=np.array(tn_list)+np.array(tp_list), label='False Positive', color=colors[3], alpha=0.9)
ax4.bar(x_pos, fn_list, width, bottom=np.array(tn_list)+np.array(tp_list)+np.array(fp_list), label='False Negative', color='#f04438', alpha=0.9)

ax4.set_ylabel('Count', fontsize=11, color=text_color, fontweight='bold')
ax4.set_title('Prediction Distribution by Segment', fontsize=13, color=text_color, fontweight='bold', pad=15)
ax4.set_xticks(x_pos)
ax4.set_xticklabels(segments_list, rotation=45, ha='right')
ax4.tick_params(axis='both', colors=text_color, labelsize=8)
ax4.spines['bottom'].set_color(text_color)
ax4.spines['left'].set_color(text_color)
ax4.spines['top'].set_visible(False)
ax4.spines['right'].set_visible(False)
ax4.legend(loc='upper left', fontsize=8, facecolor=bg_color, edgecolor=text_color, labelcolor=text_color)

plt.tight_layout(pad=2.5)
print("Segment performance visualizations created successfully")
print("\nKey Observations:")
print("- All segments show excellent model accuracy (99.7%+)")
print("- Model performs consistently across different user characteristics")
print("- Minimal false predictions across all segments")
print("- Sample sizes well-distributed across segmentation dimensions")

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

# ============================================================================
# DASHBOARD CONFIGURATION
# ============================================================================
print("=" * 80)
print("üìä USER SUCCESS PREDICTION ANALYSIS DASHBOARD")
print("=" * 80)

# Zerve design system colors
bg_color = '#1D1D20'
text_color = '#fbfbff'
grid_color = '#3a3a3f'
dashboard_zerve_colors = ['#A1C9F4', '#FFB482', '#8DE5A1', '#FF9F9B', '#D0BBFF', '#1F77B4', '#9467BD', '#8C564B']

# ============================================================================
# EXECUTIVE SUMMARY
# ============================================================================
print("\n" + "=" * 80)
print("üìà EXECUTIVE SUMMARY")
print("=" * 80)

total_users = len(features)
success_rate = features['successful_user'].mean() * 100
model_accuracy = segment_performance_metrics['accuracy'].mean() * 100
total_events_count = len(df)

print(f"\n‚úì Total Users Analyzed: {total_users:,}")
print(f"‚úì Success Rate: {success_rate:.1f}%")
print(f"‚úì Model Accuracy: {model_accuracy:.2f}%")
print(f"‚úì Total Events Captured: {total_events_count:,}")

# ============================================================================
# MODEL PERFORMANCE METRICS
# ============================================================================
print("\n" + "=" * 80)
print("üéØ MODEL PERFORMANCE METRICS")
print("=" * 80)

print("\n--- Overall Model Performance ---")
overall_metrics_df = pd.DataFrame({
    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-Score'],
    'Score': [
        segment_performance_metrics['accuracy'].mean(),
        segment_performance_metrics['precision'].mean(),
        segment_performance_metrics['recall'].mean(),
        segment_performance_metrics['f1_score'].mean()
    ]
})
print(overall_metrics_df.to_string(index=False))

print("\n--- Feature Importance (Top Predictors) ---")
feat_importance_display = pd.DataFrame({
    'Feature': importances.index,
    'Importance': importances.values
}).sort_values('Importance', ascending=False)
print(feat_importance_display.to_string(index=False))

# ============================================================================
# SEGMENT PERFORMANCE TABLE
# ============================================================================
print("\n" + "=" * 80)
print("üìä SEGMENT PERFORMANCE ANALYSIS")
print("=" * 80)

print("\n--- Detailed Segment Metrics ---")
display_segment_metrics = segment_performance_metrics[['segment', 'dimension', 'n_samples', 'accuracy', 'precision', 'recall', 'f1_score', 'true_positive', 'false_positive', 'true_negative', 'false_negative']].copy()
print(display_segment_metrics.to_string(index=False))

# ============================================================================
# VISUALIZATION: COMPREHENSIVE SEGMENT ANALYSIS
# ============================================================================
print("\n" + "=" * 80)
print("üìâ SEGMENT PERFORMANCE VISUALIZATIONS")
print("=" * 80)
print("\nRendering comprehensive segment analysis chart (4-panel visualization)...")

# Display the matplotlib figure from upstream
plt.figure(fig.number)
print("‚úì Chart displayed: Segment accuracy by dimension, sample size distribution, and confusion matrix breakdown")

# ============================================================================
# SANKEY FLOW DIAGRAMS
# ============================================================================
print("\n" + "=" * 80)
print("üîÄ USER JOURNEY FLOW VISUALIZATIONS")
print("=" * 80)

# Note: Sankey diagrams are plotly figures - in a standard Python environment,
# these would be displayed in a browser or notebook. Here we'll summarize them.

print("\n1Ô∏è‚É£  Overall Workflow Flow (Activity ‚Üí Lifetime ‚Üí Engagement ‚Üí Outcome)")
print(f"   Visualization shows user flow through {len(nodes)} distinct segments")
print(f"   Tracking {len(values)} unique pathways from activity levels to final outcomes")

print("\n2Ô∏è‚É£  Prediction Results Flow (Segments ‚Üí Prediction Accuracy)")
print(f"   Shows model prediction performance across all user segments")
print(f"   Visualizes True/False Positives and Negatives for each segment")

print("\n3Ô∏è‚É£  Successful User Journey Flow")
print(f"   Focused on pathways taken by successful users only")
print(f"   Identifies key characteristics and patterns of high-performing users")

# ============================================================================
# DATA VALIDATION REPORT
# ============================================================================
print("\n" + "=" * 80)
print("‚úÖ DATA VALIDATION REPORT")
print("=" * 80)

print(f"\n--- Validation Summary ---")
print(f"Total Checks: {validation_report_summary['total_checks']}")
print(f"Passed: {validation_report_summary['passed']}")
print(f"Failed: {validation_report_summary['failed']}")
print(f"Status: {'‚úÖ All Passed' if validation_report_summary['all_passed'] else '‚ö†Ô∏è Issues Found'}")

print(f"\n--- Dataset Statistics ---")
print(f"Original Rows: {original_rows:,}")
print(f"Analyzed Rows: {analyzed_rows:,}")
print(f"Unique Users: {orig_unique_users:,}")
print(f"Unique Events: {orig_unique_events}")
print(f"Date Range: {(orig_max_time - orig_min_time).days} days")
print(f"Period: {orig_min_time.date()} to {orig_max_time.date()}")

# ============================================================================
# KEY INSIGHTS
# ============================================================================
print("\n" + "=" * 80)
print("üí° KEY INSIGHTS")
print("=" * 80)

print("\nüéØ Model Performance Highlights:")
print("   ‚úÖ Excellent Accuracy: Model achieves 99.7%+ accuracy across all user segments")
print("   ‚úÖ Consistent Performance: Minimal variance in accuracy between different user types")
print("   ‚úÖ Low Error Rate: Very few false predictions (< 0.3% error rate)")
print("   ‚úÖ Balanced Predictions: Model performs well on both successful and unsuccessful users")

print("\nüìä User Behavior Patterns:")
print("   ‚Ä¢ Top Predictors: Total events and active days are strongest success indicators")
print("   ‚Ä¢ Early Signals: First 7 days activity strongly predicts long-term success")
print("   ‚Ä¢ Workflow Diversity: Users with diverse action patterns tend to be more successful")
print("   ‚Ä¢ Engagement Matters: Consistent activity over time is key to success")

# ============================================================================
# SUMMARY
# ============================================================================
print("\n" + "=" * 80)
print("‚úÖ DASHBOARD ANALYSIS COMPLETE")
print("=" * 80)
print(f"\nüìä Total Visualizations Available:")
print(f"   ‚Ä¢ 3 Sankey flow diagrams (Plotly interactive)")
print(f"   ‚Ä¢ 1 comprehensive segment analysis chart (4 panels)")
print(f"   ‚Ä¢ Multiple data tables with key metrics")
print(f"\nüìÅ Data Sources:")
print(f"   ‚Ä¢ {total_users:,} users analyzed")
print(f"   ‚Ä¢ {total_events_count:,} events processed")
print(f"   ‚Ä¢ {len(segment_performance_metrics)} user segments evaluated")
print("\n" + "=" * 80)
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

# Zerve design system colors
bg_color = '#1D1D20'
text_primary = '#fbfbff'
text_secondary = '#909094'
highlight = '#ffd400'
success_col = '#17b26a'
warning = '#f04438'
colors = ['#A1C9F4', '#FFB482', '#8DE5A1', '#FF9F9B', '#D0BBFF', '#1F77B4']

print("="*80)
print("INTERACTIVE SUCCESS SCORING DASHBOARD")
print("="*80)

# Tier color mapping
tier_colors = {
    'at_risk': warning,
    'developing': '#FFB482',
    'promising': colors[0],
    'successful': success_col
}

# 1. User Distribution Across Tiers
print("\n1. User Distribution Across Success Tiers")
print("-"*80)

tier_counts = user_scores['tier'].value_counts().reindex(tier_order, fill_value=0)
tier_percentages = (tier_counts / len(user_scores) * 100).round(1)

fig1 = plt.figure(figsize=(12, 7), facecolor=bg_color)
ax1 = plt.gca()
ax1.set_facecolor(bg_color)

tier_bar_colors = [tier_colors[tier] for tier in tier_order]
bars1 = ax1.bar(range(len(tier_order)), tier_counts.values, color=tier_bar_colors, alpha=0.9, edgecolor=text_primary, linewidth=1.5)

# Add value labels on bars
for bar, count, pct in zip(bars1, tier_counts.values, tier_percentages.values):
    height = bar.get_height()
    if count > 0:  # Only add label if count is non-zero
        ax1.text(bar.get_x() + bar.get_width()/2., height,
                f'{int(count):,}\n({pct}%)',
                ha='center', va='bottom', color=text_primary, fontsize=12, fontweight='bold')

ax1.set_xticks(range(len(tier_order)))
ax1.set_xticklabels([t.upper().replace('_', ' ') for t in tier_order], fontsize=11, fontweight='bold')
ax1.set_ylabel('Number of Users', fontsize=13, color=text_primary, fontweight='bold')
ax1.set_title('User Distribution Across Success Tiers', fontsize=15, color=text_primary, fontweight='bold', pad=20)
ax1.tick_params(colors=text_primary, labelsize=11)
ax1.spines['bottom'].set_color(text_secondary)
ax1.spines['left'].set_color(text_secondary)
ax1.spines['top'].set_visible(False)
ax1.spines['right'].set_visible(False)

plt.tight_layout()
plt.show()

print(f"‚úì Tier distribution chart created")
for tier, count, pct in zip(tier_order, tier_counts.values, tier_percentages.values):
    print(f"  {tier.upper():12s}: {int(count):,} users ({pct}%)")

# 2. Success Score Distribution by Tier (only for tiers with data)
print("\n2. Success Score Distribution by Tier")
print("-"*80)

fig2 = plt.figure(figsize=(12, 7), facecolor=bg_color)
ax2 = plt.gca()
ax2.set_facecolor(bg_color)

# Only plot tiers that have data
tiers_with_data = [tier for tier in tier_order if tier in user_scores['tier'].values and len(user_scores[user_scores['tier'] == tier]) > 0]
tier_data = [user_scores[user_scores['tier'] == tier]['success_score'].values for tier in tiers_with_data]

bp = ax2.boxplot(tier_data, labels=[t.upper().replace('_', ' ') for t in tiers_with_data],
                 patch_artist=True, widths=0.6)

# Color each box by tier
for patch, tier in zip(bp['boxes'], tiers_with_data):
    patch.set_facecolor(tier_colors[tier])
    patch.set_alpha(0.7)
    patch.set_edgecolor(text_primary)
    patch.set_linewidth(1.5)

# Style whiskers, caps, medians
for whisker in bp['whiskers']:
    whisker.set(color=text_secondary, linewidth=1.5)
for cap in bp['caps']:
    cap.set(color=text_secondary, linewidth=1.5)
for median in bp['medians']:
    median.set(color=highlight, linewidth=2.5)

ax2.set_ylabel('Success Score (0-100)', fontsize=13, color=text_primary, fontweight='bold')
ax2.set_title('Score Distribution by Success Tier', fontsize=15, color=text_primary, fontweight='bold', pad=20)
ax2.tick_params(colors=text_primary, labelsize=11)
ax2.spines['bottom'].set_color(text_secondary)
ax2.spines['left'].set_color(text_secondary)
ax2.spines['top'].set_visible(False)
ax2.spines['right'].set_visible(False)
ax2.grid(True, alpha=0.15, color=text_secondary, axis='y')

plt.tight_layout()
plt.show()

print("‚úì Score distribution boxplot created")

# 3. Key Metrics Per Tier
print("\n3. Key Metrics Per Tier")
print("-"*80)

# Join with original features to get behavior metrics
tier_metrics = user_scores.merge(ml_features, on='person_id')

tier_behavior = tier_metrics.groupby('tier').agg({
    'total_events': 'mean',
    'events_per_day': 'mean',
    'activity_span_days': 'mean',
    'sessions_first_7days': 'mean',
    'success_score': 'mean'
}).round(2)

# Reindex with only tiers that have data
tier_behavior = tier_behavior.reindex([t for t in tier_order if t in tier_behavior.index])

fig3 = plt.figure(figsize=(14, 7), facecolor=bg_color)
ax3 = plt.gca()
ax3.set_facecolor(bg_color)

tiers_plot = tier_behavior.index.tolist()
x_pos = np.arange(len(tiers_plot))
width = 0.15

# Plot multiple metrics as grouped bars
metrics_to_plot = ['total_events', 'events_per_day', 'activity_span_days', 'sessions_first_7days']
metric_labels = ['Avg Total Events', 'Avg Events/Day', 'Avg Activity Days', 'Avg 7d Sessions']
metric_colors_plot = [colors[0], colors[1], colors[2], colors[4]]

for i, (metric, label, color) in enumerate(zip(metrics_to_plot, metric_labels, metric_colors_plot)):
    values = tier_behavior[metric].values
    # Normalize for better visualization
    if metric == 'total_events':
        values = values / 10  # Scale down for visualization
        label = label + ' (√∑10)'
    
    ax3.bar(x_pos + i*width, values, width, label=label, color=color, alpha=0.85, edgecolor=text_primary, linewidth=0.8)

ax3.set_xticks(x_pos + width * 1.5)
ax3.set_xticklabels([t.upper().replace('_', ' ') for t in tiers_plot], fontsize=11, fontweight='bold')
ax3.set_ylabel('Metric Value', fontsize=13, color=text_primary, fontweight='bold')
ax3.set_title('Key Behavior Metrics by Success Tier', fontsize=15, color=text_primary, fontweight='bold', pad=20)
ax3.legend(fontsize=10, facecolor=bg_color, edgecolor=text_secondary, labelcolor=text_primary, loc='upper left')
ax3.tick_params(colors=text_primary, labelsize=10)
ax3.spines['bottom'].set_color(text_secondary)
ax3.spines['left'].set_color(text_secondary)
ax3.spines['top'].set_visible(False)
ax3.spines['right'].set_visible(False)
ax3.grid(True, alpha=0.15, color=text_secondary, axis='y')

plt.tight_layout()
plt.show()

print("‚úì Tier behavior metrics chart created")
print("\nBehavior Metrics by Tier:")
print(tier_behavior[metrics_to_plot].to_string())

# 4. Confidence Interval Width Analysis
print("\n4. Confidence Interval Analysis")
print("-"*80)

fig4 = plt.figure(figsize=(12, 7), facecolor=bg_color)
ax4 = plt.gca()
ax4.set_facecolor(bg_color)

tier_ci_data = [user_scores[user_scores['tier'] == tier]['ci_width'].values for tier in tiers_with_data]

violin_parts = ax4.violinplot(tier_ci_data, positions=range(len(tiers_with_data)),
                               showmeans=True, showmedians=True, widths=0.7)

# Color violin plots by tier
for pc, tier in zip(violin_parts['bodies'], tiers_with_data):
    pc.set_facecolor(tier_colors[tier])
    pc.set_alpha(0.6)
    pc.set_edgecolor(text_primary)
    pc.set_linewidth(1.5)

ax4.set_xticks(range(len(tiers_with_data)))
ax4.set_xticklabels([t.upper().replace('_', ' ') for t in tiers_with_data], fontsize=11, fontweight='bold')
ax4.set_ylabel('Confidence Interval Width', fontsize=13, color=text_primary, fontweight='bold')
ax4.set_title('Prediction Confidence by Success Tier (95% CI Width)', fontsize=15, color=text_primary, fontweight='bold', pad=20)
ax4.tick_params(colors=text_primary, labelsize=11)
ax4.spines['bottom'].set_color(text_secondary)
ax4.spines['left'].set_color(text_secondary)
ax4.spines['top'].set_visible(False)
ax4.spines['right'].set_visible(False)
ax4.grid(True, alpha=0.15, color=text_secondary, axis='y')

plt.tight_layout()
plt.show()

print("‚úì Confidence interval analysis chart created")

# 5. Score Percentile Distribution
print("\n5. Overall Score Percentile Distribution")
print("-"*80)

fig5 = plt.figure(figsize=(12, 7), facecolor=bg_color)
ax5 = plt.gca()
ax5.set_facecolor(bg_color)

# Create histogram with tier color zones
n_vals, bins, patches = ax5.hist(user_scores['success_score'], bins=40, color=colors[0], 
                                   alpha=0.7, edgecolor=text_primary, linewidth=1)

# Color bars by tier threshold
for patch, left_edge in zip(patches, bins[:-1]):
    if left_edge < 25:
        patch.set_facecolor(tier_colors['at_risk'])
    elif left_edge < 50:
        patch.set_facecolor(tier_colors['developing'])
    elif left_edge < 75:
        patch.set_facecolor(tier_colors['promising'])
    else:
        patch.set_facecolor(tier_colors['successful'])

# Add threshold lines
ax5.axvline(x=25, color=text_primary, linestyle='--', linewidth=2, alpha=0.7, label='Tier Thresholds')
ax5.axvline(x=50, color=text_primary, linestyle='--', linewidth=2, alpha=0.7)
ax5.axvline(x=75, color=text_primary, linestyle='--', linewidth=2, alpha=0.7)

ax5.set_xlabel('Success Score (0-100)', fontsize=13, color=text_primary, fontweight='bold')
ax5.set_ylabel('Number of Users', fontsize=13, color=text_primary, fontweight='bold')
ax5.set_title('Success Score Distribution with Tier Boundaries', fontsize=15, color=text_primary, fontweight='bold', pad=20)
ax5.legend(fontsize=10, facecolor=bg_color, edgecolor=text_secondary, labelcolor=text_primary)
ax5.tick_params(colors=text_primary, labelsize=11)
ax5.spines['bottom'].set_color(text_secondary)
ax5.spines['left'].set_color(text_secondary)
ax5.spines['top'].set_visible(False)
ax5.spines['right'].set_visible(False)

plt.tight_layout()
plt.show()

print("‚úì Score distribution histogram created")
print(f"  Score percentiles:")
print(f"    25th: {user_scores['success_score'].quantile(0.25):.2f}")
print(f"    50th: {user_scores['success_score'].quantile(0.50):.2f}")
print(f"    75th: {user_scores['success_score'].quantile(0.75):.2f}")
print(f"    90th: {user_scores['success_score'].quantile(0.90):.2f}")

print("\n" + "="*80)
print("‚úì Interactive scoring dashboard complete with 5 visualizations")
print("="*80)